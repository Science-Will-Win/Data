{"task_id": "eval1_gwas_variant_prioritization_134", "benchmark": "biomni-eval1", "task_type": "gwas_variant_prioritization", "question": "Your task is to identify the most promising variant associated wtih a given GWAS phenotype for futher examination. \nFrom the list, prioritize the top associated variant (matching one of the given variant). \nGWAS phenotype: Bradykinin\nVariants: rs7700133, rs1280, rs7651090, rs4253311, rs3738934, rs7385804, rs1367117, rs4808136, rs10087900, rs855791, rs12678919\n", "ground_truth": "rs4253311", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "NameError: name 're' is not defined", "metadata": {"execution_time_sec": 226.77, "timestamp": "2026-02-06T11:12:03.377426+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 234, in run\n    final_answer=extract_clean_answer(str(answer) if answer else \"\", task.get(\"task_type\", \"\")),\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 44, in extract_clean_answer\n    sol = re.search(r'<solution>\\s*(.*?)\\s*</solution>', text, re.DOTALL)\n          ^^\nNameError: name 're' is not defined\n"}}
{"task_id": "eval1_gwas_variant_prioritization_178", "benchmark": "biomni-eval1", "task_type": "gwas_variant_prioritization", "question": "Your task is to identify the most promising variant associated wtih a given GWAS phenotype for futher examination. \nFrom the list, prioritize the top associated variant (matching one of the given variant). \nGWAS phenotype: Homocysteine\nVariants: rs9369898, rs1395, rs662138, rs2011069, rs6742078, rs4939883, rs4503368, rs12550729, rs7529794, rs2100944, rs6678639\n", "ground_truth": "rs9369898", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "NameError: name 're' is not defined", "metadata": {"execution_time_sec": 162.15, "timestamp": "2026-02-06T11:19:35.445410+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 234, in run\n    final_answer=extract_clean_answer(str(answer) if answer else \"\", task.get(\"task_type\", \"\")),\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 44, in extract_clean_answer\n    sol = re.search(r'<solution>\\s*(.*?)\\s*</solution>', text, re.DOTALL)\n          ^^\nNameError: name 're' is not defined\n"}}
{"task_id": "eval1_gwas_variant_prioritization_197", "benchmark": "biomni-eval1", "task_type": "gwas_variant_prioritization", "question": "Your task is to identify the most promising variant associated wtih a given GWAS phenotype for futher examination. \nFrom the list, prioritize the top associated variant (matching one of the given variant). \nGWAS phenotype: erythritol\nVariants: rs2000999, rs6687813, rs6678639, rs7542172, rs10903129, rs1801133, rs1570669, rs1697421, rs13375749, rs1171614, rs12709013\n", "ground_truth": "rs7542172", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "NameError: name 're' is not defined", "metadata": {"execution_time_sec": 424.74, "timestamp": "2026-02-06T11:41:34.913638+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 234, in run\n    final_answer=extract_clean_answer(str(answer) if answer else \"\", task.get(\"task_type\", \"\")),\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 44, in extract_clean_answer\n    sol = re.search(r'<solution>\\s*(.*?)\\s*</solution>', text, re.DOTALL)\n          ^^\nNameError: name 're' is not defined\n"}}
{"task_id": "eval1_lab_bench_seqqa_213", "benchmark": "biomni-eval1", "task_type": "lab_bench_seqqa", "question": "The following is a multiple choice question about biology.\nPlease answer by responding with the letter of the correct answer.\n\nQuestion: I want to clone the mlaE gene from E. coli into the plasmid pUC19. I'm going to linearize the plasmid with HindII. Which primer pair can I use to amplify the gene for Gibson assembly into the linearized vector?\nOptions:\nA.Insufficient information to answer the question.\nB.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGCTGGAAAATTTGAATCTCTCTCTAT, GAGCTCGGTACCCGGGGATCCTCTAGAGTCTCAGTCACGCACCCAGCCTTTGCGGATC\nC.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGCTGTTAAATGCGCTGGCGT, GAGCTCGGTACCCGGGGATCCTCTAGAGTCTCAATTCCCAAACATCAATGCGGTC\nD.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGTGCTGTTAAATGCGCTGGCG, GAGCTCGGTACCCGGGGATCCTCTAGAGTCCAATTCCCAAACATCAATGC\nE.ATGCCTGCAGGTCGACTCTAGAGGATCCCCATGCTGTTAAATGCGCTGGCGT, CGACGGCCAGTGAATTCGAGCTCGGTACCCTCAATTCCCAAACATCAATGCGGTC\n\nYou MUST include the letter of the correct answer within the following tags:\n[ANSWER] and [/ANSWER]. For example, '[ANSWER]<answer>[/ANSWER]',\nwhere <answer> is the correct letter. Always answer in exactly this format\nof a single letter between the two tags, even if you are unsure.\nWe require this because we use automatic parsing.\n            ", "ground_truth": "C", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33565 input tokens (8192 > 40960 - 33565). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 394.21, "timestamp": "2026-02-07T11:47:23.619552+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33565 input tokens (8192 > 40960 - 33565). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id 'a3ff991f-08f2-d7be-d910-5d114a7e745c'\n"}}
{"task_id": "eval1_lab_bench_seqqa_267", "benchmark": "biomni-eval1", "task_type": "lab_bench_seqqa", "question": "The following is a multiple choice question about biology.\nPlease answer by responding with the letter of the correct answer.\n\nQuestion: I want to clone the ubiI gene from E. coli into the plasmid pUC19. I'm going to linearize the plasmid with SmaI. Which primer pair can I use to amplify the gene for Gibson assembly into the linearized vector?\nOptions:\nA.ATGCCTGCAGGTCGACTCTAGAGGATCCCCATGCAGATCCTCAAAGTGATCGGCC, CGACGGCCAGTGAATTCGAGCTCGGTACCCTCATTTCCCACCTCCCGCACTGATGTCG\nB.Insufficient information to answer the question.\nC.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGCAAAGTGTTGATGTAGCCATTGTT, GAGCTCGGTACCCGGGGATCCTCTAGAGTCTTAACGCAGCCATTCAGGCA\nD.ATGCCTGCAGGTCGACTCTAGAGGATCCCCATGCAAAGTGTTGATGTAGCCATTGTT, CGACGGCCAGTGAATTCGAGCTCGGTACCCTTAACGCAGCCATTCAGGCA\nE.ATGCCTGCAGGTCGACTCTAGAGGATCCCCATGGCAAAGTGTTGATGTAGCCATTGT, CGACGGCCAGTGAATTCGAGCTCGGTACCCTAACGCAGCCATTCAGGCAAATCGTT\n\nYou MUST include the letter of the correct answer within the following tags:\n[ANSWER] and [/ANSWER]. For example, '[ANSWER]<answer>[/ANSWER]',\nwhere <answer> is the correct letter. Always answer in exactly this format\nof a single letter between the two tags, even if you are unsure.\nWe require this because we use automatic parsing.\n            ", "ground_truth": "D", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 35323 input tokens (8192 > 40960 - 35323). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 410.2, "timestamp": "2026-02-07T13:08:54.051387+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 35323 input tokens (8192 > 40960 - 35323). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id '6cff95c7-6378-f948-eed3-cbc365ef14e8'\n"}}
{"task_id": "eval1_lab_bench_seqqa_230", "benchmark": "biomni-eval1", "task_type": "lab_bench_seqqa", "question": "The following is a multiple choice question about biology.\nPlease answer by responding with the letter of the correct answer.\n\nQuestion: I want to clone the waaA gene from E. coli into the plasmid pUC19. I'm going to linearize the plasmid with HindII. Which primer pair can I use to amplify the gene for Gibson assembly into the linearized vector?\nOptions:\nA.Insufficient information to answer the question.\nB.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGTTAGATATAGTCGAACTG, GAGCTCGGTACCCGGGGATCCTCTAGAGTCTTAGCGTGCCGGCTGAGTAGTCGT\nC.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGCTCGAATTGCTTTACACCGCC, GAGCTCGGTACCCGGGGATCCTCTAGAGTCTCAATGCGTTTTCGGTGGCAGG\nD.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGGCTCGAATTGCTTTACAC, GAGCTCGGTACCCGGGGATCCTCTAGAGTCAATGCGTTTTCGGTGGCAGG\nE.ATGCCTGCAGGTCGACTCTAGAGGATCCCCATGCTCGAATTGCTTTACACCGCC, CGACGGCCAGTGAATTCGAGCTCGGTACCCTCAATGCGTTTTCGGTGGCAGG\n\nYou MUST include the letter of the correct answer within the following tags:\n[ANSWER] and [/ANSWER]. For example, '[ANSWER]<answer>[/ANSWER]',\nwhere <answer> is the correct letter. Always answer in exactly this format\nof a single letter between the two tags, even if you are unsure.\nWe require this because we use automatic parsing.\n            ", "ground_truth": "C", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33614 input tokens (8192 > 40960 - 33614). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 546.92, "timestamp": "2026-02-07T14:17:07.036133+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33614 input tokens (8192 > 40960 - 33614). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id 'da8bbec3-1427-52e4-2f92-f1ff924f1749'\n"}}
{"task_id": "eval1_lab_bench_seqqa_216", "benchmark": "biomni-eval1", "task_type": "lab_bench_seqqa", "question": "The following is a multiple choice question about biology.\nPlease answer by responding with the letter of the correct answer.\n\nQuestion: I want to clone the plsB gene from E. coli into the plasmid pUC19. I'm going to linearize the plasmid with HindII. Which primer pair can I use to amplify the gene for Gibson assembly into the linearized vector?\nOptions:\nA.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGGTCCGGCTGGCCACGAATTTACTACA, GAGCTCGGTACCCGGGGATCCTCTAGAGTCTACCCTTCGCCCTGCGTCGCACT\nB.ATGCCTGCAGGTCGACTCTAGAGGATCCCCATGTCCGGCTGGCCACGAATTTACTACA, CGACGGCCAGTGAATTCGAGCTCGGTACCCTTACCCTTCGCCCTGCGTCGCAC\nC.Insufficient information to answer the question.\nD.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGATTCGCTTAGCGCCCTTGATTAC, GAGCTCGGTACCCGGGGATCCTCTAGAGTCTTAGGCGAAAATATCAACGCC\nE.GATTACGCCAAGCTTGCATGCCTGCAGGTCATGTCCGGCTGGCCACGAATTTACTACA, GAGCTCGGTACCCGGGGATCCTCTAGAGTCTTACCCTTCGCCCTGCGTCGCAC\n\nYou MUST include the letter of the correct answer within the following tags:\n[ANSWER] and [/ANSWER]. For example, '[ANSWER]<answer>[/ANSWER]',\nwhere <answer> is the correct letter. Always answer in exactly this format\nof a single letter between the two tags, even if you are unsure.\nWe require this because we use automatic parsing.\n            ", "ground_truth": "E", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 37445 input tokens (8192 > 40960 - 37445). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 600.95, "timestamp": "2026-02-07T15:56:58.588782+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 37445 input tokens (8192 > 40960 - 37445). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id 'da44e080-ac42-c29b-872b-5b0a9c875205'\n"}}
{"task_id": "eval1_patient_gene_detection_217", "benchmark": "biomni-eval1", "task_type": "patient_gene_detection", "question": "\nTask: Given a patient's phenotypes and a list of candidate genes, identify the causal gene.\nPhenotypes: HP:0002355, HP:0005109, HP:0007178, HP:0010867, HP:0001336, HP:0003326, HP:0001251, HP:0000708, HP:0012443, HP:0001257, HP:0010286, HP:0001279, HP:0025421, HP:0001746, HP:0010953, HP:0011852, HP:0031923\nCandidate genes: ENSG00000163873, ENSG00000167114, ENSG00000171105, ENSG00000106692, ENSG00000170927, ENSG00000144381, ENSG00000170540, ENSG00000160710, ENSG00000130702, ENSG00000189221, ENSG00000077235, ENSG00000163794, ENSG00000186472, ENSG00000106344, ENSG00000115339, ENSG00000119537\n\nOutput format: {'causal_gene': [gene1]}\n        ", "ground_truth": "ENSG00000170540", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 35035 input tokens (8192 > 40960 - 35035). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 1440.33, "timestamp": "2026-02-07T20:36:41.985246+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 35035 input tokens (8192 > 40960 - 35035). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id 'abadad99-68c5-a888-afc9-fbb0cabe2d44'\n"}}
{"task_id": "eval1_patient_gene_detection_1", "benchmark": "biomni-eval1", "task_type": "patient_gene_detection", "question": "\nTask: Given a patient's phenotypes and a list of candidate genes, identify the causal gene.\nPhenotypes: HP:0000982, HP:0004751, HP:0010719, HP:0000113, HP:0001324, HP:0001640, HP:0003202, HP:0000708, HP:0011675, HP:0001948, HP:0000956, HP:0001645, HP:0200114, HP:0005881, HP:0025230, HP:0003010, HP:0000540\nCandidate genes: ENSG00000115760, ENSG00000168000, ENSG00000136732, ENSG00000166603, ENSG00000162426, ENSG00000130638, ENSG00000116679, ENSG00000070614, ENSG00000090932, ENSG00000082701, ENSG00000157423, ENSG00000185339, ENSG00000073756, ENSG00000133107, ENSG00000147655, ENSG00000173801, ENSG00000117425, ENSG00000055118, ENSG00000122641, ENSG00000115361, ENSG00000166685\n\nOutput format: {'causal_gene': [gene1]}\n        ", "ground_truth": "ENSG00000173801", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33467 input tokens (8192 > 40960 - 33467). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 352.64, "timestamp": "2026-02-07T22:16:43.121549+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33467 input tokens (8192 > 40960 - 33467). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id '0dd68f30-f5b4-47b0-7423-24d94330dd30'\n"}}
{"task_id": "eval1_patient_gene_detection_473", "benchmark": "biomni-eval1", "task_type": "patient_gene_detection", "question": "\nTask: Given a patient's phenotypes and a list of candidate genes, identify the causal gene.\nPhenotypes: HP:0000518, HP:0000528, HP:0000568, HP:0001250, HP:0000525, HP:0001159, HP:3000050, HP:0000968, HP:0008070, HP:0000028, HP:0000639, HP:0000545, HP:0006101, HP:0100256, HP:0001640, HP:0030154, HP:0001818, HP:0200025, HP:0000533\nCandidate genes: ENSG00000175426, ENSG00000176046, ENSG00000060237, ENSG00000270765, ENSG00000132563, ENSG00000179941, ENSG00000049167, ENSG00000125378, ENSG00000168646, ENSG00000170289, ENSG00000177606, ENSG00000001497, ENSG00000139618, ENSG00000135346, ENSG00000141431, ENSG00000164961, ENSG00000141646, ENSG00000197785, ENSG00000077463, ENSG00000213281\n\nOutput format: {'causal_gene': [gene1]}\n        ", "ground_truth": "ENSG00000125378", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33424 input tokens (8192 > 40960 - 33424). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 1181.42, "timestamp": "2026-02-08T04:08:26.947840+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33424 input tokens (8192 > 40960 - 33424). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id 'c892ed6d-fd49-c4d6-30c8-c74c22d8c004'\n"}}
{"task_id": "eval1_patient_gene_detection_387", "benchmark": "biomni-eval1", "task_type": "patient_gene_detection", "question": "\nTask: Given a patient's phenotypes and a list of candidate genes, identify the causal gene.\nPhenotypes: HP:0001249, HP:0002421, HP:0002465, HP:0012537, HP:0002244, HP:0000565, HP:0011015, HP:0001943, HP:0001272, HP:0001336, HP:0007420, HP:0002119, HP:0003040, HP:0001627, HP:0030424, HP:0001788\nCandidate genes: ENSG00000132394, 387583, ENSG00000126749, ENSG00000134014, ENSG00000105048, ENSG00000019549, ENSG00000077498, ENSG00000125871, ENSG00000100485, ENSG00000141527, ENSG00000258366, ENSG00000117620, ENSG00000167674, ENSG00000079215, ENSG00000117305, ENSG00000168036, ENSG00000213380, ENSG00000112964, ENSG00000158158, ENSG00000126218, ENSG00000143178, ENSG00000104447, ENSG00000101194\n\nOutput format: {'causal_gene': [gene1]}\n        ", "ground_truth": "ENSG00000213380", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33786 input tokens (8192 > 40960 - 33786). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 1355.56, "timestamp": "2026-02-08T06:12:32.384069+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33786 input tokens (8192 > 40960 - 33786). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id '93352735-1bff-ada6-e216-0f32e8e242be'\n"}}
{"task_id": "eval1_rare_disease_diagnosis_90", "benchmark": "biomni-eval1", "task_type": "rare_disease_diagnosis", "question": "\nTask: given a patient's phenotypes and a list of candidate genes, diagnose the rare disease that the patient has.\nPhenotypes: HP:0002650, HP:0000175, HP:0007099, HP:0004322, HP:0001263, HP:0009473\nCandidate genes: ['ENSG00000154864']\n\nOutput format: {'disease_name': XXX, 'OMIM_ID': XXX}\n        ", "ground_truth": "{\"disease_name\": \"Gordon syndrome\", \"OMIM_ID\": \"114300\"}", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 32930 input tokens (8192 > 40960 - 32930). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 857.94, "timestamp": "2026-02-08T19:21:20.594740+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 32930 input tokens (8192 > 40960 - 32930). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id '42cd43c3-3d85-11e9-748e-4fc0c9b7c58f'\n"}}
{"task_id": "eval1_rare_disease_diagnosis_88", "benchmark": "biomni-eval1", "task_type": "rare_disease_diagnosis", "question": "\nTask: given a patient's phenotypes and a list of candidate genes, diagnose the rare disease that the patient has.\nPhenotypes: HP:0000508, HP:0000193, HP:0009473, HP:0000347\nCandidate genes: ['ENSG00000154864']\n\nOutput format: {'disease_name': XXX, 'OMIM_ID': XXX}\n        ", "ground_truth": "{\"disease_name\": \"Gordon syndrome\", \"OMIM_ID\": \"114300\"}", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 34938 input tokens (8192 > 40960 - 34938). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 465.42, "timestamp": "2026-02-08T20:44:09.275068+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 34938 input tokens (8192 > 40960 - 34938). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id 'b3ed65e1-a135-a02e-7892-985bbf0be1a1'\n"}}
{"task_id": "eval1_rare_disease_diagnosis_136", "benchmark": "biomni-eval1", "task_type": "rare_disease_diagnosis", "question": "\nTask: given a patient's phenotypes and a list of candidate genes, diagnose the rare disease that the patient has.\nPhenotypes: HP:0001249\nCandidate genes: ['ENSG00000101825']\n\nOutput format: {'disease_name': XXX, 'OMIM_ID': XXX}\n        ", "ground_truth": "{\"disease_name\": \"autism (disease)\", \"OMIM_ID\": \"209850\"}", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33234 input tokens (8192 > 40960 - 33234). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 498.19, "timestamp": "2026-02-08T23:17:28.191572+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33234 input tokens (8192 > 40960 - 33234). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id 'a32128ad-c91a-c1f5-f097-554e444cefc6'\n"}}
{"task_id": "eval1_rare_disease_diagnosis_1", "benchmark": "biomni-eval1", "task_type": "rare_disease_diagnosis", "question": "\nTask: given a patient's phenotypes and a list of candidate genes, diagnose the rare disease that the patient has.\nPhenotypes: HP:0002000, HP:0000205, HP:0002987, HP:0000343, HP:0002086, HP:0000347, HP:0003457, HP:0000293, HP:0002650, HP:0001181, HP:0002793, HP:0001762, HP:0003273, HP:0001250, HP:0002093, HP:0001270, HP:0006380, HP:0000470, HP:0010751, HP:0012385, HP:0001371, HP:0000431, HP:0011824\nCandidate genes: ['ENSG00000102452']\n\nOutput format: {'disease_name': XXX, 'OMIM_ID': XXX}\n        ", "ground_truth": "{\"disease_name\": \"Congenital Contractures of the Limbs and Face, Hypotonia, and Developmental Delay\", \"OMIM_ID\": \"616266\"}", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 36223 input tokens (8192 > 40960 - 36223). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 146.49, "timestamp": "2026-02-08T23:32:29.289296+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 36223 input tokens (8192 > 40960 - 36223). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id '5390f638-260c-353d-9542-775504b2f745'\n"}}
{"task_id": "eval1_rare_disease_diagnosis_116", "benchmark": "biomni-eval1", "task_type": "rare_disease_diagnosis", "question": "\nTask: given a patient's phenotypes and a list of candidate genes, diagnose the rare disease that the patient has.\nPhenotypes: HP:0009473, HP:0001488, HP:0002091, HP:0002650, HP:0005745\nCandidate genes: ['ENSG00000154864']\n\nOutput format: {'disease_name': XXX, 'OMIM_ID': XXX}\n        ", "ground_truth": "{\"disease_name\": \"Arthrogryposis with oculomotor limitation and electroretinal anomalies\", \"OMIM_ID\": \"108145\"}", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 32833 input tokens (8192 > 40960 - 32833). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 386.21, "timestamp": "2026-02-09T02:36:46.850261+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 32833 input tokens (8192 > 40960 - 32833). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id '77e6237a-a0ac-ebf3-194f-8752c23be165'\n"}}
{"task_id": "eval1_rare_disease_diagnosis_87", "benchmark": "biomni-eval1", "task_type": "rare_disease_diagnosis", "question": "\nTask: given a patient's phenotypes and a list of candidate genes, diagnose the rare disease that the patient has.\nPhenotypes: HP:0000602, HP:0000175, HP:0009473, HP:0000347\nCandidate genes: ['ENSG00000154864']\n\nOutput format: {'disease_name': XXX, 'OMIM_ID': XXX}\n        ", "ground_truth": "{\"disease_name\": \"Gordon syndrome\", \"OMIM_ID\": \"114300\"}", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33212 input tokens (8192 > 40960 - 33212). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}", "metadata": {"execution_time_sec": 758.01, "timestamp": "2026-02-09T03:36:19.440449+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1780, in go\n    for s in self.app.stream(inputs, stream_mode=\"values\", config=config):\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2646, in stream\n    for _ in runner.tick(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n    ret = self.func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1390, in generate\n    response = self.llm.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 402, in invoke\n    self.generate_prompt(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1121, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 931, in generate\n    self._generate_with_cache(\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1233, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1386, in _generate\n    raise e\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1381, in _generate\n    raw_response = self.client.with_raw_response.create(**payload)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n                                      ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 286, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1192, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1294, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/openai/_base_client.py\", line 1067, in request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'max_tokens' or 'max_completion_tokens' is too large: 8192. This model's maximum context length is 40960 tokens and your request has 33212 input tokens (8192 > 40960 - 33212). (parameter=max_tokens, value=8192)\", 'type': 'BadRequestError', 'param': 'max_tokens', 'code': 400}}\nDuring task with name 'generate' and id '9f1d4271-c46a-e2a6-00bc-dbb2ae4da857'\n"}}
{"task_id": "eval1_rare_disease_diagnosis_145", "benchmark": "biomni-eval1", "task_type": "rare_disease_diagnosis", "question": "\nTask: given a patient's phenotypes and a list of candidate genes, diagnose the rare disease that the patient has.\nPhenotypes: HP:0001250, HP:0001263, HP:0000047, HP:0000639, HP:0000821, HP:0000238, HP:0001513\nCandidate genes: ['ENSG00000101871']\n\nOutput format: {'disease_name': XXX, 'OMIM_ID': XXX}\n        ", "ground_truth": "{\"disease_name\": \"Opitz Gbbb Syndrome, Type 1\", \"OMIM_ID\": \"300000\"}", "trajectory": [], "final_answer": "", "score": -1.0, "success": false, "error": "BrokenPipeError: [Errno 32] Broken pipe", "metadata": {"execution_time_sec": 67.52, "timestamp": "2026-02-09T03:31:03.363051+00:00", "traceback": "Traceback (most recent call last):\n  File \"/home/sww/aigen-bioagent/trajectory_collection/run_trajectory_collection.py\", line 219, in run\n    conv, answer = self.agent.go(task[\"prompt\"])\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/agent/a1.py\", line 1782, in go\n    out = pretty_print(message)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sww/.conda/envs/biomni-agent/lib/python3.11/site-packages/biomni/utils.py\", line 462, in pretty_print\n    print(f\"{title}\")\nBrokenPipeError: [Errno 32] Broken pipe\n"}}
