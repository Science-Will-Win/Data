**AIGEN BioAgent**

**Phase2 팀 질문답변 02 --- 2026.02.19**

  --------------- -------------------------------------------------------
  **항목**        **내용**

  질문자          구재현 (ki_ctree)

  답변자          박정훈 (Phase 1 리드)

  작성일          2026-02-19

  맥락            GitHub Data 저장소에 공유된 학습 데이터 및 모델
                  체크포인트에 대한 Phase 2 팀 질의
  --------------- -------------------------------------------------------

Q1. 이거 학습 LoRA로 하셨어요?

**답변: 예, 모든 학습에 LoRA 계열 방식을 사용했습니다.**

  ---------------- ------------- ----------------------------------------
  **학습**         **방식**      **상세**

  GDPO (선호도     QLoRA 4-bit   Qwen3-32B 베이스, 4비트 양자화 + LoRA
  학습)                          

  Ministral 3B SFT LoRA          전체 파라미터 대비 소량만 학습

  Uncertainty Head MLP 학습      모델 본체는 고정(freeze), 헤드만 학습
  ---------------- ------------- ----------------------------------------

GDPO 학습의 LoRA 설정:

  -------------------------- --------------------------------------------
  **항목**                   **값**

  rank (차원 축소 크기)      128

  alpha (학습 강도 조절)     128

  target_modules             q_proj, k_proj, v_proj, o_proj, gate_proj,
                             up_proj, down_proj

  전체 파라미터              33.8B

  학습 파라미터              1,074M (전체의 3.17%)
  -------------------------- --------------------------------------------

> **💡 비유:** 33.8B(338억 개) 파라미터를 가진 모델 전체를 학습하면 GPU
> 메모리가 부족합니다. LoRA는 \"모델의 핵심 부분만 살짝 조정하는
> 방식\"으로, 전체의 3.17%만 학습하여 메모리를 절약하면서도 성능 개선
> 효과를 얻습니다.

Q2. 학습을 Eval1으로 하신거에요? data가 433개네요?

**답변: 아닙니다. 학습 종류마다 데이터가 다릅니다.**

  ---------------- ---------- ---------------------- ----------------------
  **학습**         **데이터   **데이터 출처**        **설명**
                   수**                              

  GDPO (선호도     829쌍      Eval1 287쌍 +          \"좋은 답변 vs 나쁜
  학습)                       LAB-Bench 542쌍        답변\" 비교 쌍

  Uncertainty Head 433건      Eval1 전체             정답/오답 라벨 +
                                                     should_refuse 라벨

  Ministral 3B SFT 444건      Eval1 정답 trajectory  정답을 맞춘 풀이
                                                     과정만 사용
  ---------------- ---------- ---------------------- ----------------------

433이라는 숫자는 Eval1 벤치마크의 전체 태스크 수입니다. GDPO 학습에는
이보다 많은 829쌍(Eval1 + LAB-Bench 합산)을 사용했습니다.

> **💡 비유:** GDPO 쌍 구성 원리: 동일 태스크에 대해 R0(우리 모델)와
> GPT-4.1이 각각 풀이한 결과를 비교하여, 한쪽이 맞고 다른 쪽이 틀린 경우
> \"정답 풀이(chosen) vs 오답 풀이(rejected)\" 쌍을 구성합니다.

Q3. MC sample의 갯수는 얼마로 설정하셨나요?

**답변: MC (Monte Carlo) dropout 방식을 사용하지 않았습니다.**

Uncertainty Head는 MC dropout이 아니라 MLP(다층 퍼셉트론) 기반의
결정론적 분류기입니다.

  --------------- -------------------------------------------------------
  **항목**        **내용**

  아키텍처        Linear(4096→256) → ReLU → Dropout(0.3) → Linear(256→64)
                  → ReLU → Linear(64→1) → Sigmoid

  총 파라미터     2,945개

  입력            모델 마지막 층의 hidden state (4096차원)

  출력            0\~1 사이의 불확실성 점수
  --------------- -------------------------------------------------------

MC dropout은 추론 시 dropout을 여러 번 켜서 \"흔들어보는\" 방식인데,
우리는 그 대신 단일 forward pass로 불확실성을 직접 예측하는 방식을
택했습니다. 속도가 빠르고 배포가 간단합니다.

> **💡 비유:** MC dropout = 같은 시험을 여러 번 봐서 답이 흔들리는
> 정도를 측정. 우리 방식 = 한 번만 봐도 \"이건 자신 없다\"고 직접
> 판단하는 능력을 학습.

Q4. Uncertainty Head를 여러 layer로 구성한 이유?

**답변: 복잡한 패턴을 학습하기 위해서입니다.**

  -------------------------------- ---------- -------------------------------------
  **구조**                         **레이어   **문제점**
                                   수**       

  Linear(4096→1)                   1개        단순 선형 분류만 가능

  Linear→ReLU→Linear→ReLU→Linear   3개        비선형 패턴 학습 가능 --- feature 간
                                              복합 관계 포착
  -------------------------------- ---------- -------------------------------------

3단계 구조의 역할:

4096 → 256 : 4096차원 hidden state에서 \"불확실성에 관련된 256개 핵심
특징\" 추출

256 → 64 : 256개 특징을 더 압축하여 패턴 조합 학습

64 → 1 : 최종 불확실성 점수 (0\~1) 출력

> **💡 비유:** 1개 레이어 = \"체온만 보고 아프다/안아프다 판단\". 3개
> 레이어 = \"체온 + 혈압 + 증상 조합을 종합적으로 고려하여 판단\".

추가로 Dropout(0.3)은 과적합(같은 데이터만 외우는 현상) 방지를 위해
넣었습니다. 433건은 적은 데이터이므로 과적합 위험이 있기 때문입니다.

Q5. GDPO로 학습을 하신건가요?

**답변: 예, GDPO 학습을 완료했습니다.**

  ---------------------- ------------------------------------------------
  **항목**               **값**

  데이터                 829쌍 (학습 724쌍 / 검증 81쌍)

  학습 step              273 steps (3 에포크)

  최종 eval accuracy     88.9%

  학습 시간              약 2시간 30분

  베이스 모델            Qwen3-32B

  학습 방식              DPOTrainer (trl 라이브러리) + QLoRA 4-bit
  ---------------------- ------------------------------------------------

학습 결과:

  --------------- --------- ----------------------------------------------
  **지표**        **값**    **해석**

  eval accuracy   88.9%     검증 데이터에서 좋은/나쁜 답변을 88.9%
                            정확도로 구분

  train loss      0.0078    학습 데이터에 대한 손실 (낮을수록 좋음)

  eval loss       0.0219    검증 데이터에 대한 손실 (train과 차이 작아
                            과적합 아님)
  --------------- --------- ----------------------------------------------

> **💡 비유:** GDPO(Group Direct Preference Optimization): 일반 DPO와
> 달리, 보상을 개별 정규화 → 합산 → 배치 정규화하여 다목적 최적화가
> 가능한 방식. trl 라이브러리의 DPOTrainer를 기반으로 구현했습니다.

Q6. 지금 데이터는 어떤게 업데이트 된건가요?

**답변: GitHub Data 저장소(Science-Will-Win/Data)에 총 21파일이
푸시되었습니다.**

커밋: bb5cb7d (2026-02-19)

  ------------------ ---------------------------- ----------------------------
  **카테고리**       **파일**                     **내용**

  Eval1 trajectory   R0/eval1_jsonl/\*.jsonl      R0 Eval1 433건 전체
                                                  trajectory

  LAB-Bench          R0/labbench_jsonl/\*.jsonl   R0 LAB-Bench 1,848건
  trajectory                                      trajectory

  GPT trajectory     GPT/eval1_jsonl/\*.jsonl     GPT-4.1 Eval1 433건
                                                  trajectory

  GDPO 쌍            gdpo/\*.jsonl                829쌍 (Eval1 287 + LAB-Bench
                                                  542)

  Uncertainty 라벨   uncertainty_labels.json      433건 should_refuse +
                                                  difficulty

  SFT 데이터         sft_data.json                444건 정답 trajectory
  ------------------ ---------------------------- ----------------------------

Q7. 모델은 /raid/sww에 넣어주세요

**답변: 조치하겠습니다.**

  --------------------- -------------------------------------------- ------------------------------------
  **모델**              **원본 경로**                                **이동 경로**

  GDPO LoRA 어댑터      /home/sww/\.../gdpo_output/final/            /raid/sww/gdpo_lora/

  GDPO 병합 모델 (FP16, /home/sww/models/Qwen3-32B-GDPO-merged/      /raid/sww/Qwen3-32B-GDPO-merged/
  62GB)                                                              

  Uncertainty Head      /home/sww/\.../uncertainty_head_output/      /raid/sww/uncertainty_head_output/

  Ministral 3B SFT      /home/sww/\.../ministral_sft_output/final/   /raid/sww/ministral_sft_output/
  --------------------- -------------------------------------------- ------------------------------------

> **💡 비유:** /raid vs /home 차이: /raid는 대용량 공유 스토리지(RAID
> 디스크), /home은 개인 작업 공간. 모델처럼 큰 파일은 /raid에 두어야
> 다른 팀원도 접근 가능하고 /home 용량도 아낄 수 있습니다.

Q8. System prompt가 왜 논의한거랑 다르게 들어가 있나요?

**답변: 현재는 Biomni-R0의 기본 system prompt를 사용하고 있습니다.**

Phase E 3조건 비교 실험 시 사용한 system prompt는 Biomni 프레임워크에
내장된 기본 프롬프트입니다. 팀 미팅에서 논의된 커스텀 system prompt가
아직 반영되지 않은 상태입니다.

  ---------------- ------------------------------------------------------
  **구분**         **내용**

  현재 사용 중     Biomni-R0 기본 프롬프트 (biomni 패키지 내장)

  팀 논의 내용     커스텀 system prompt (도구 사용 지침, 거부 규칙 등
                   추가)

  차이 원인        Phase 1 데이터 수집 시 일관된 조건 유지를 위해 기본
                   프롬프트로 통일
  ---------------- ------------------------------------------------------

향후 조치: 팀에서 확정한 커스텀 system prompt가 있으면 공유해주시면,
추가 실험이나 재수집 시 반영하겠습니다.

Q9. Loss 정의부는 어디에 있나요?

**답변: 각 학습 스크립트 내부에 정의되어 있습니다.**

  ---------------- --------------------------- -------------------------------
  **학습**         **파일**                    **Loss 위치**

  GDPO             train_gdpo.py               DPOTrainer 내부 자동 정의 (trl
                                               라이브러리)

  Uncertainty Head train_uncertainty_head.py   nn.BCELoss() (이진 교차
                                               엔트로피)

  Ministral 3B SFT train_ministral_sft.py      SFTTrainer 내부 자동 정의 (trl
                                               라이브러리)
  ---------------- --------------------------- -------------------------------

각 Loss 설명:

**GDPO Loss (DPO 방식):**

Loss = -log(σ(β × (log π(chosen) - log π(rejected))))

해석: \"좋은 답변의 확률은 높이고, 나쁜 답변의 확률은 낮추는\" 방향으로
학습

β(beta) = 0.1 (보수적 학습, 기존 모델에서 크게 벗어나지 않도록)

**Uncertainty Head Loss (BCELoss):**

Loss = -(y × log(ŷ) + (1-y) × log(1-ŷ))

y = 정답 라벨 (1=거부해야 함, 0=답변 가능)

ŷ = 모델 예측 (0\~1 사이 불확실성 점수)

**Ministral SFT Loss (Cross Entropy):**

Loss = -Σ log P(다음 토큰 \| 이전 토큰들)

해석: 정답 trajectory의 각 토큰을 올바르게 예측하도록 학습

Q10. Inference 부분 코드도 같은 방식으로 작성되어 있나요?

**답변: 학습과 추론은 다른 구조이지만, 같은 모델/설정을 공유합니다.**

  ------------- ---------------------------- ----------------------------
  **구분**      **학습 (Training)**          **추론 (Inference)**

  프레임워크    trl (DPOTrainer, SFTTrainer) vLLM 서버

  모델 로드     QLoRA 4-bit + LoRA 어댑터    AWQ-INT4 양자화 또는 병합
                                             FP16

  GPU           2개 (6-7번)                  2개 (6-7번)

  배치 처리     배치 학습 (batch=1,          동시 요청 처리 (vLLM 자동
                grad_accum=8)                배칭)
  ------------- ---------------------------- ----------------------------

추론 시 vLLM으로 서빙하는 구조:

모델 로드 (vLLM)

↓

OpenAI 호환 API 서버 실행 (http://localhost:8000/v1)

↓

Biomni 에이전트가 API를 호출하여 태스크 수행

↓

Trajectory 로그 저장

GDPO 학습된 모델을 추론에 사용하려면:

1\) LoRA 어댑터를 베이스 모델에 병합 (이미 완료:
/raid/sww/Qwen3-32B-GDPO-merged/)

2\) 병합된 모델을 vLLM으로 서빙

3\) Biomni 에이전트의 base_url을 vLLM 서버 주소로 설정

코드 파일 위치:

  ---------------------- ----------------------------------------------------
  **용도**               **파일**

  GDPO 학습              /home/sww/aigen-bioagent/train_gdpo.py

  Uncertainty Head 학습  /home/sww/aigen-bioagent/train_uncertainty_head.py

  Ministral SFT 학습     /home/sww/aigen-bioagent/train_ministral_sft.py

  Phase E 비교 실험      /home/sww/aigen-bioagent/run_phase_e.py
  (추론)                 

  vLLM 서빙              tmux 세션에서 직접 실행
  ---------------------- ----------------------------------------------------

용어 주석

  ---------------------------- ------------------------------------------
  **용어**                     **설명**

  LoRA (Low-Rank Adaptation)   대형 모델의 일부 파라미터만 학습하여
                               메모리를 절약하는 기법

  QLoRA (Quantized LoRA)       LoRA + 4비트 양자화를 결합하여 더 적은
                               메모리로 학습

  GDPO (Group Direct           \"좋은 답변 vs 나쁜 답변\" 비교를 통해
  Preference Optimization)     모델을 개선하는 학습 방식

  SFT (Supervised Fine-Tuning) 정답 데이터를 보고 따라하는 지도 학습

  MLP (Multi-Layer Perceptron) 여러 층으로 구성된 기본 신경망

  BCELoss (Binary Cross        이진 분류(맞다/아니다)를 위한 손실 함수
  Entropy Loss)                

  MC Dropout (Monte Carlo      추론 시 dropout을 반복하여 불확실성을
  Dropout)                     추정하는 방법

  vLLM                         대형 언어모델을 빠르게 서빙하기 위한 추론
                               엔진

  hidden state                 모델 내부의 중간 표현 벡터 (4096차원)

  Eval1                        Biomni-Eval1 벤치마크 (433개 바이오의학
                               태스크)

  LAB-Bench                    실험실 수준 바이오 추론 벤치마크 (1,967개
                               태스크, 8개 서브셋)

  trajectory                   에이전트가 문제를 푸는 전체 과정
                               (생각→행동→관찰→답변)

  Phase E                      3조건(Base R0 / GDPO / GDPO+Uncertainty)
                               비교 실험

  /raid                        서버의 대용량 공유 스토리지 (RAID 구성
                               디스크)
  ---------------------------- ------------------------------------------
